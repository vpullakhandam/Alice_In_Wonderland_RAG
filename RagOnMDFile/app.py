import streamlit as st
import sys
import os
import json
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from pydantic import BaseModel, Field, field_validator
from typing import List, Optional

# SQLite compatibility shim for Streamlit Cloud (use pysqlite3 if available)
try:
    import pysqlite3 as _sqlite3  # type: ignore
    sys.modules["sqlite3"] = _sqlite3
    sys.modules["_sqlite3"] = _sqlite3
except Exception:
    pass
try:
    from langchain_chroma import Chroma as ChromaStore
    CHROMA_DEPRECATED = False
except Exception:
    from langchain_community.vectorstores import Chroma as ChromaStore
    CHROMA_DEPRECATED = True
from langchain_huggingface import HuggingFaceEmbeddings
try:
    from FlagEmbedding import FlagReranker
except Exception:
    FlagReranker = None

# Pydantic models for structured output
class Expansions(BaseModel):
    items: List[str] = Field(..., min_items=1, description="List of paraphrased questions")
    
    @field_validator('items')
    @classmethod
    def validate_items(cls, v):
        if not all(isinstance(item, str) and len(item.strip()) > 0 for item in v):
            raise ValueError("All items must be non-empty strings")
        return [item.strip() for item in v]

class RetrievedMetadata(BaseModel):
    source: str
    chapter_number: Optional[str] = None
    chapter_title: Optional[str] = None
    position: Optional[int] = Field(None, alias="start_index")
    chunk_number: Optional[int] = None
    chunk_summary: Optional[str] = None

class RetrievedChunk(BaseModel):
    question: str
    score: float
    content: str
    metadata: RetrievedMetadata

class RetrievalResults(BaseModel):
    results: List[RetrievedChunk]

class RerankedDocument(BaseModel):
    content: str
    metadata: RetrievedMetadata
    rerank_score: float
    
class RerankedResults(BaseModel):
    results: List[RerankedDocument]
    original_question: str
    model_name: str = "BAAI/bge-reranker-base"

class Citation(BaseModel):
    source: str
    chapter_number: Optional[str] = None
    chapter_title: Optional[str] = None
    position: Optional[int] = Field(None, alias="start_index")
    chunk_number: Optional[int] = None

class GeminiResponse(BaseModel):
    answer: str = Field(..., description="The answer from Gemini")
    citations: List[Citation] = Field(default_factory=list, description="Citations from the context")

# Setup page config
st.set_page_config(
    page_title="Alice in Wonderland Q&A",
    page_icon="🎩",
    layout="wide"
)

# Constants
PERSIST_DIR = os.path.join(os.path.dirname(__file__), "chroma_store")
COLLECTION = "alice"

def get_api_key_from_secrets() -> str:
    return st.secrets.get("GEMINI_API_KEY", "")

def get_api_key_from_env() -> str:
    return os.getenv("GEMINI_API_KEY", "")

def check_api_key():
    """Check if API key is properly set (prefer Streamlit Secrets by default)."""
    # Default: use Streamlit Secrets
    api_key = get_api_key_from_secrets()

    # Local development option: comment the above and uncomment the next line
    # api_key = get_api_key_from_env()

    if not api_key:
        st.error("⚠️ GEMINI_API_KEY not found. Add it in Settings → Secrets (Cloud) or .env (local).")
        return False
    os.environ["GEMINI_API_KEY"] = api_key
    os.environ["GOOGLE_API_KEY"] = api_key
    return True

@st.cache_resource
def get_reranker():
    """Get or initialize reranker with proper error handling."""
    if FlagReranker is None:
        return None
    try:
        import torch
        from transformers import AutoModel
        
        # Force CPU and disable gradients
        os.environ["CUDA_VISIBLE_DEVICES"] = ""
        torch.set_grad_enabled(False)
        
        # Initialize model with empty weights first
        with torch.device('meta'):
            meta_model = AutoModel.from_pretrained(
                "BAAI/bge-reranker-base",
                torch_dtype=torch.float32,
                trust_remote_code=True
            )
        
        # Move to CPU and materialize weights
        model = meta_model.to_empty(device='cpu')
        model.load_state_dict(meta_model.state_dict())
        
        # Create reranker with materialized model
        reranker = FlagReranker(
            "BAAI/bge-reranker-base",
            use_fp16=False,
            device="cpu",
            model=model  # Use our properly initialized model
        )
        return reranker
    except Exception:
        return None

@st.cache_resource
def load_vectordb():
    """Load the persisted Chroma DB built in the notebook using BGE embeddings."""
    try:
        sqlite_path = os.path.join(PERSIST_DIR, "chroma.sqlite3")
        if not os.path.isdir(PERSIST_DIR) or not os.path.exists(sqlite_path):
            st.error("Chroma store not found. Run the notebook to build 'chroma_store/'.")
            return None
        embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-base-en-v1.5")
        vectordb = ChromaStore(
            embedding_function=embeddings,
            persist_directory=PERSIST_DIR,
            collection_name=COLLECTION,
        )
        if CHROMA_DEPRECATED:
            st.warning("Using deprecated Chroma import from langchain_community. Consider installing 'langchain-chroma' and switching to the new import.")
        return vectordb
    except Exception as e:
        st.error(f"Error loading Chroma DB: {str(e)}")
        return None

def expand_queries(llm: ChatGoogleGenerativeAI, question: str, n: int = 4) -> List[str]:
    """Generate query expansions using Gemini with structured output."""
    prompt = f"""
    You are helping to search Alice in Wonderland by generating variations of a question.
    Generate {n} different ways to ask this question that will help find relevant passages.
    Include variations with character names (e.g., "Mad Hatter" and "Hatter") and different phrasings.
    
    Return ONLY a valid JSON object with this exact format:
    {{"items": ["paraphrase1", "paraphrase2", ...]}}

    Question: "{question}"

    Tips:
    - Include character name variations
    - Try different event descriptions
    - Consider scene-specific words
    - Mix specific and general phrasings

    Remember: Return ONLY the JSON object, no other text.
    """
    response = llm.invoke(prompt)
    try:
        # Try to parse the response as JSON
        json_str = response.content.strip()
        # Find JSON object if it's embedded in other text
        start = json_str.find('{')
        end = json_str.rfind('}') + 1
        if start >= 0 and end > start:
            json_str = json_str[start:end]
        result = json.loads(json_str)
        
        # Validate with Pydantic
        expansions = Expansions(items=result.get('items', []))
        # Ensure we have exactly n items
        items = expansions.items[:n] if len(expansions.items) > n else expansions.items + [question] * (n - len(expansions.items))
        return items
    except Exception:
        # Fallback: return original question repeated
        return [question] * n

def retrieve_candidates(vectordb, queries, per_query_k: int = 5) -> RetrievalResults:
    """Retrieve candidates with structured output. Always returns exactly per_query_k chunks per query."""
    results = []
    seen = set()
    
    # Ensure we have exactly 4 queries
    queries = queries[:4] if len(queries) > 4 else queries + [queries[0]] * (4 - len(queries))
    
    for q in queries:
        query_results = []
        hits = vectordb.similarity_search_with_score(q, k=per_query_k * 2)  # Get extra to handle duplicates
        
        for doc, score in hits:
            key = (doc.metadata.get('start_index'), doc.metadata.get('chunk_number'))
            if key not in seen and len(query_results) < per_query_k:
                seen.add(key)
                chunk = RetrievedChunk(
                    question=q,
                    score=float(score),
                    content=doc.page_content,
                    metadata=RetrievedMetadata(**doc.metadata)
                )
                query_results.append(chunk)
        
        # If we still don't have enough unique results, relax the uniqueness constraint
        if len(query_results) < per_query_k:
            for doc, score in hits:
                if len(query_results) < per_query_k:
                    chunk = RetrievedChunk(
                        question=q,
                        score=float(score),
                        content=doc.page_content,
                        metadata=RetrievedMetadata(**doc.metadata)
                    )
                    query_results.append(chunk)
        
        results.extend(query_results[:per_query_k])  # Ensure exactly per_query_k results per query
    
    return RetrievalResults(results=results)

def rerank_candidates(question: str, candidates: RetrievalResults, top_n: int = 5) -> RerankedResults:
    """Rerank candidates with structured output."""
    reranker = get_reranker()
    
    def get_similarity_ranked_docs():
        """Helper to get similarity-score ranked documents."""
        sorted_candidates = sorted(candidates.results, key=lambda x: x.score)
        return [
            RerankedDocument(
                content=doc.content,
                metadata=doc.metadata,
                rerank_score=float(doc.score)
            ) for doc in sorted_candidates[:top_n]
        ]
    
    if reranker is None:
        return RerankedResults(
            results=get_similarity_ranked_docs(),
            original_question=question
        )

    try:
        # Clean and validate inputs
        pairs = []
        valid_docs = []
        
        for doc in candidates.results:
            if not doc.content or not question:
                continue
            # Clean and truncate text
            doc_text = " ".join(doc.content.split())[:4096]  # Limit doc length
            q_text = " ".join(question.split())[:512]  # Limit query length
            if len(doc_text) < 10 or len(q_text) < 3:  # Skip very short texts
                continue
            pairs.append([q_text, doc_text])
            valid_docs.append(doc)
        
        if not pairs:
            return RerankedResults(
                results=get_similarity_ranked_docs(),
                original_question=question
            )
            
        # Process in small batches
        scores = reranker.compute_score(pairs, batch_size=4)
        reranked = sorted(zip(valid_docs, scores), key=lambda x: x[1], reverse=True)
        
        # Build final results
        seen = set()
        top_docs = []
        for doc, rerank_score in reranked:
            sid = doc.metadata.position
            if sid not in seen:
                seen.add(sid)
                reranked_doc = RerankedDocument(
                    content=doc.content,
                    metadata=doc.metadata,
                    rerank_score=float(rerank_score)
                )
                top_docs.append(reranked_doc)
            if len(top_docs) == top_n:
                break
        
        return RerankedResults(results=top_docs, original_question=question)
        
    except Exception as e:
        st.warning(f"Reranking failed, falling back to similarity scores. Error: {str(e)}")
        # Fallback to similarity scores
        sorted_candidates = sorted(candidates.results, key=lambda x: x.score)
        reranked_docs = [
            RerankedDocument(
                content=doc.content,
                metadata=doc.metadata,
                rerank_score=float(doc.score)
            ) for doc in sorted_candidates[:top_n]
        ]
        return RerankedResults(results=reranked_docs, original_question=question)

def build_context(docs: RerankedResults) -> tuple[str, List[Citation]]:
    """Build context with structured output."""
    parts = []
    citations = []
    
    for doc in docs.results:
        m = doc.metadata
        # Add minimal header for context building
        header = f"[Chapter {m.chapter_number} {m.chapter_title} | Position={m.position}]"
        parts.append(header + "\n" + doc.content)
        
        citations.append(Citation(
            source=m.source,
            chapter_number=m.chapter_number,
            chapter_title=m.chapter_title,
            start_index=m.position,
            chunk_number=m.chunk_number
        ))
    
    return "\n\n".join(parts), citations

def get_answer(
    vectordb,
    query: str,
    use_reranker: bool = True,
    per_query_k: int = 5,
    final_top_n: int = 8,  # Fixed at 8 chunks
) -> GeminiResponse:
    """Get answer with structured output."""
    try:
        # Initialize Gemini with more robust settings
        llm_gemini = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=0.3,
            top_p=0.95,  # Increased for more diverse responses
            top_k=40,    # Increased for more options
            max_output_tokens=1024  # Increased for longer responses
        )

        # 1. Query Expansion
        expansions = expand_queries(llm_gemini, query, n=4)
        queries = [query] + expansions

        # 2. Initial Retrieval
        candidates = retrieve_candidates(vectordb, queries, per_query_k=per_query_k)
        if not candidates.results:
            return GeminiResponse(
                answer="I couldn't find any relevant information in the book about that. Could you try rephrasing your question?",
                citations=[]
            )

        # 3. Reranking
        # Rerank or use similarity scores
        if use_reranker and candidates.results:
            top_docs = rerank_candidates(query, candidates, top_n=min(final_top_n, len(candidates.results)))
        else:
            # Fallback to ANN scores
            sorted_candidates = sorted(candidates.results, key=lambda x: x.score)
            reranked_docs = [
                RerankedDocument(
                    content=doc.content,
                    metadata=doc.metadata,
                    rerank_score=float(doc.score)
                ) for doc in sorted_candidates[:final_top_n]
            ]
            top_docs = RerankedResults(results=reranked_docs, original_question=query)

        if not top_docs.results:
            return GeminiResponse(
                answer="I couldn't find enough relevant information. Could you try rephrasing your question?",
                citations=[]
            )

        # 4. Context Building
        context, citations = build_context(top_docs)
        
        # Enhanced prompt for better answers
        prompt = PromptTemplate.from_template(
            """
            You are a helpful assistant answering questions about Alice in Wonderland.
            Use ONLY the provided context to answer the question.
            
            Guidelines:
            1. If the context contains the information, provide a clear and complete answer
            2. If the context doesn't contain enough information, say so
            3. Focus on accuracy and completeness
            4. Use 2-3 sentences minimum
            5. Stay under 100 words unless more detail is needed
            6. Only use quotes if specifically asked
            7. Always provide a full, finished answer
            
            Question: {question}
            
            Context:
            {context}
            
            Remember: Base your answer ONLY on the provided context, not general knowledge.
            """
        )

        # 5. Final Answer Generation
        chat = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=0.2,  # Lower temperature for more focused answers
            top_p=0.85,
            top_k=30,
            max_output_tokens=1024  # Increased for longer responses
        )
        response = chat.invoke(prompt.format(question=query, context=context))
        
        # Validate response
        if not response.content or len(response.content.strip()) < 10:
            return GeminiResponse(
                answer="I found relevant information but had trouble generating a good answer. Please try asking again, perhaps with different wording.",
                citations=citations
            )
        
        return GeminiResponse(
            answer=response.content,
            citations=citations
        )
    except Exception as e:
        st.error(f"Error: {str(e)}")
        return GeminiResponse(
            answer="I encountered an error while processing your question. Please try asking again with different wording.",
            citations=[]
        )

def main():
    st.title("🎩 Alice in Wonderland Q&A")
    st.markdown(
        "Discover answers grounded in the original text of Alice in Wonderland.\n\n"
        "Ask a question in the box below. The app searches a persistent knowledge base built from the book,"
        " expands your query, reranks the most relevant passages, and answers using only the retrieved context."
    )

    # Check API key first
    if not check_api_key():
        return

    # Load persisted vector DB
    with st.spinner("Loading knowledge base (Chroma)..."):
        vectordb = load_vectordb()
        if vectordb is None:
            return

    # Sidebar: instructions and examples
    with st.sidebar:
        st.header("How to use")
        st.markdown(
            "- **Type a question** about the story.\n"
            "- **Or click a sample question** to try.\n"
            "- Optionally **show sources** to see which chunks were used."
        )

        st.subheader("Settings")
        show_sources = st.checkbox("Show retrieved sources", value=True)

        st.subheader("Try a sample")
        examples = [
            "Why did Alice follow the White Rabbit?",
            "What happens at the tea party with the Mad Hatter?",
            "How does Alice meet the Mad Hatter?",
            "Describe the Cheshire Cat's appearance under 50 words?",
        ]
        if "user_query" not in st.session_state:
            st.session_state.user_query = ""
        for ex in examples:
            if st.button(ex):
                st.session_state.user_query = ex
                st.rerun()  # Use the new st.rerun() instead of experimental_rerun

    # Main interaction
    query = st.text_input(
        "Ask your question",
        key="user_query",
        placeholder="e.g., Why did Alice follow the White Rabbit?",
    )

    if query:
        with st.spinner("Finding answer..."):
            response = get_answer(
                vectordb,
                query,
                use_reranker=True,  # Always use reranker
                final_top_n=8,  # Fixed at 8 chunks
            )
        st.markdown("### Answer")
        st.write(response.answer)

        if show_sources and response.citations:
            with st.expander("📚 Source Passages"):
                for citation in response.citations:
                    st.markdown(f"**Chapter {citation.chapter_number} - {citation.chapter_title}**")
                    st.markdown(f"*Position {citation.position}, Chunk {citation.chunk_number}*")

if __name__ == "__main__":
    main()